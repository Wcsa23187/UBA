Namespace(GMF_model_path='None', MLP_model_path='None', batch_size=2048, data_path='F:\\RSlib\\code\\Recommender_baselines-main\\data', data_type='time', dataset='electronics', dropout=0.0, epochs=100, factor_num=32, gpu='0', log_name='log', lr=0.001, model='NeuMF-end', model_path='./models/', num_layers=5, num_ng=4, out=True, test_num_ng=99, top_k='[10, 20, 50, 100]')
--init--
Epoch [000]  loss is 4.697
Epoch [001]  loss is 0.937
Epoch [002]  loss is 0.887
Epoch [003]  loss is 0.881
Epoch [004]  loss is 0.864
Epoch [005]  loss is 0.830
Epoch [006]  loss is 0.785
Epoch [007]  loss is 0.742
Epoch [008]  loss is 0.699
Epoch [009]  loss is 0.660
vt_dict 8999
mse tensor(0.9077, device='cuda:0', dtype=torch.float64)
vt_dict 9496
mse tensor(1.2767, device='cuda:0', dtype=torch.float64)
Validation: The time elapse of epoch 009 is: 00: 00: 13
GENERATE FAKE
Traceback (most recent call last):
  File "main.py", line 297, in <module>
    generate_target_result(model, train_df)
  File "main.py", line 183, in generate_target_result
    test_predRatings_torch = model(test_uids_torch, test_iids_torch)
  File "D:\anacando\lib\site-packages\torch\nn\modules\module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "F:\RSlib\code\Recommender_baselines-main\NCF\model.py", line 123, in forward
    embed_item_MLP = self.embed_item_MLP(i  tem)
  File "D:\anacando\lib\site-packages\torch\nn\modules\module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\anacando\lib\site-packages\torch\nn\modules\sparse.py", line 160, in forward
    self.norm_type, self.scale_grad_by_freq, self.sparse)
  File "D:\anacando\lib\site-packages\torch\nn\functional.py", line 2199, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: CUDA out of memory. Tried to allocate 2.83 GiB (GPU 0; 6.00 GiB total capacity; 3.42 GiB already allocated; 880.18 MiB free; 3.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
